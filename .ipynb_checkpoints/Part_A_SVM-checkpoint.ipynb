{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A: Linear Support Vector Machine (SVM)\n",
    "Dataset: You will use the Iris dataset for binary classification. Use petal length and\n",
    "petal width features of the Iris dataset, and determine whether a sample is Iris Virginica\n",
    "or not.\n",
    "URL: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
    "1. Implement a Linear_SVC model class for performing binary classification. The\n",
    "model should implement the batch Gradient Descent (GD) algorithm.\n",
    " [40 pts]\n",
    "a) __init__(self, C=1, max_iter=100, tol=None, learning_rate=‘constant’,\n",
    "learning_rate_init=0.001, t_0=1, t_1=1000, early_stopping=False,\n",
    "validation_fraction=0.1, **kwargs)\n",
    "This method is used to initialize the data members of the class when an object\n",
    "of class is created. For example, self.C = C\n",
    " Arguments:\n",
    "C : float\n",
    "It provides the regularization/penalty coefficient.\n",
    "max_iter : int\n",
    "Maximum number of iterations. The GD algorithm iterates until\n",
    "convergence (determined by ‘tol’) or this number of iterations.\n",
    "tol : float\n",
    "Tolerance for the optimization.\n",
    "learning_rate : string (default ‘constant’)\n",
    "It allows to specify the technique to set the learning rate: constant\n",
    "learning for all iterations, or varying learning rate given by a learning rate\n",
    "schedule function.\n",
    "3\n",
    "- ‘constant’: a constant learning rate given by ‘learning_rate_init’.\n",
    "- ‘adaptive’: gradually decreases the learning rate based on a\n",
    "schedule. Write a function that would be used if learning_rate is set\n",
    "to ‘adaptive’. When ‘adaptive’ is used, the ‘learning_rate_init’\n",
    "parameter has no effect as the learning rate varies by a learning rate\n",
    "schedule function. It uses the ‘t_0’ and ‘t_1’ parameters (see below).\n",
    "-\n",
    " Pseudocode for the “adaptive” learning_rate function:\n",
    " Write a function that decreases learning rate gradually during each iteration:\n",
    "Learning rate = !_#\n",
    "$!%&'!$()*!_+\n",
    " where t_0 and t_1 are two constants that you need to determine empirically.\n",
    " Choose constant t_0 and t_1 such that initially the learning rate is large\n",
    "enough.\n",
    "learning_rate_init : double\n",
    "The initial learning rate value if learning_rate is set to ‘constant’. It\n",
    "controls the step-size in updating the weights. It has no effect is the\n",
    "‘learning_rate’ is ‘adaptive’.\n",
    "early_stopping : Boolean, default=False\n",
    "Whether to use early stopping to terminate training when validation score\n",
    "is not improving. If set to True, it will automatically set aside a fraction\n",
    "of training data as validation and terminate training when validation\n",
    "score is not improving.\n",
    "validation_fraction : float, default=0.1\n",
    "The proportion of training data to set aside as validation set for early\n",
    "stopping. Must be between 0 and 1. Only used if early_stopping is True.\n",
    "b) fit(self, X, Y):\n",
    "Implement the batch GD algorithm in the fit method. The weight vector and the\n",
    "intercept/bias should be denoted by w and b, respectively. Store the cost values\n",
    "for each iteration so that later you can use it to create a learning curve.\n",
    "Arguments:\n",
    "X : ndarray\n",
    "A numpy array with rows representing data samples and columns\n",
    "representing features.\n",
    "Y : ndarray\n",
    "A 1D numpy array with labels corresponding to each row of the feature\n",
    "matrix X.\n",
    "Note: the “fit” method should update the following parameters:\n",
    "4\n",
    " self.intercept_ = np.array([b])\n",
    " self.coef_ = np.array([w])\n",
    " self.support_vectors_ =\n",
    " The “fit” method should display the total number of iterations using a print\n",
    "statement.\n",
    "Returns:\n",
    "self\n",
    "c)\n",
    " predict(self, X)\n",
    " Arguments:\n",
    "X : ndarray\n",
    "A numpy array containing samples to be used for prediction. Its rows\n",
    "represent data samples and columns represent features.\n",
    "Returns:\n",
    "1D array of predicted class labels for each row in X.\n",
    "Note: the “predict” method uses the self.coef_[0] and self.intercept_[0] to make\n",
    "predictions.\n",
    "Binary Classification using Linear_SVC Classifier\n",
    "2. Read the Iris data using the sklearn.datasets.load_iris method. Create the data\n",
    "matrix X by using two features: petal length and petal width. Recode the binary\n",
    "target such that Iris-Virginica samples are 1, and other samples are 0.\n",
    "[1 pts]\n",
    "3. Partition the data into train and test set (80% - 20%). Use the “Partition”\n",
    "function from your previous assignment.\n",
    "[2 pts]\n",
    "4. Model selection via Hyper-parameter tuning: Use the kFold function from\n",
    "previous assignment to find the optimal values for the following\n",
    "hyperparameters.\n",
    " [5 pts]\n",
    "\n",
    "C\n",
    "learning_rate\n",
    "learning_rate_init (when ‘constant’ learning_rate is used)\n",
    "5\n",
    "max_iter\n",
    "tol\n",
    "5. Train the model using optimal values for the hyperparameters and evaluate on the\n",
    "test data. Report the test accuracy and test confusion matrix.\n",
    "[5 pts]\n",
    "6. Plot the learning curve.\n",
    " [5 pts]\n",
    "7. Plot the decision boundary and show the support vectors using the\n",
    "“decision_boundary_support_vectors” function given in:\n",
    "https://github.com/rhasanbd/Support-Vector-Machine-Classifier-BeginnersSurvival-Kit/blob/master/Support%20Vector%20Machine-1-\n",
    "Linearly%20Separable%20Data.ipynb\n",
    " [12 pts]\n",
    "Note that if your test accuracy is less than 95% you will lose 10% of the total\n",
    "obtained points. If your test accuracy is less than 90% you will lose 30% of the\n",
    "total obtained points.\n",
    "8. [Extra Credit for 478 and Mandatory for 878] Implement early stopping in\n",
    "the “fit” method of the Linear_SVC model. You will have to use the following\n",
    "two parameters of the model: early_stopping and validation_fraction. Also note\n",
    "that when training the model using early stopping it should generate an early\n",
    "stopping curve. [10 pts] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, precision_score, recall_score, f1_score, roc_curve, precision_recall_curve, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  \n",
    "y = (iris[\"target\"] == 2).astype(np.int)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.hstack([X,y])\n",
    "np.random.shuffle(Z)\n",
    "X= Z[:,0:2]\n",
    "y = Z[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize the data\n",
    "X = (X - np.mean(X))/np.std(X)\n",
    "\n",
    "def split_trainTest(X,y,t):\n",
    "    train_size = int((1-t) * X.shape[0])   \n",
    "    return X[:train_size],X[train_size:],y[:train_size],y[train_size:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_trainTest(X,y,t=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_SVC:\n",
    "    def __init__(self, C=1, max_iter=100, tol=None, learning_rate='constant',learning_rate_init=0.001, t_0=1, t_1=500, early_stopping=False, validation_fraction=0.1,**kwargs):\n",
    "        self.C = C\n",
    "        self.w = np.random.rand(2,1)\n",
    "        self.b = 0\n",
    "        #self.w = np.reshape(np.array([[1.60381071, 3.05536121]]),(2,1))\n",
    "        #self.b = -1.13503736      \n",
    "        self.epochs = max_iter\n",
    "        self.lr_sc = learning_rate\n",
    "        self.learning_rate = learning_rate_init\n",
    "        self.tol = tol \n",
    "        self.t_0 = t_0\n",
    "        self.t_1 = t_1\n",
    "        self.loss = 1e5\n",
    "        self.intercept_ = None\n",
    "        self.coef_ = None\n",
    "        self.support_vectors_ = None\n",
    "        self.early_stopping = early_stopping\n",
    "        self.validation_fraction = validation_fraction\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        ##ADd early_stopping like Assignment 3\n",
    "        epch_counter = 0\n",
    "        \n",
    "        val_loss = []\n",
    "        self.validation_score=1e5\n",
    "        \n",
    "        if self.early_stopping:\n",
    "            X, X_valid, y, y_valid = split_trainTest(X, y, self.validation_fraction)\n",
    "        \n",
    "        # y was re-assigned into 1 and -1.\n",
    "        t = (2 * y - 1)[:,None]\n",
    "        \n",
    "        \n",
    "        while epch_counter < self.epochs:\n",
    "            loss = self.loss\n",
    "            epch_counter += 1\n",
    "            validation_score = self.validation_score\n",
    "            #print(f'Epoch: {epch_counter}')\n",
    "            #pdb.set_trace()\n",
    "            w = self.w\n",
    "            b = self.b\n",
    "            if(self.lr_sc == \"adaptive\"):\n",
    "                self.learning_rate = self.t_0 /(epch_counter + self.t_1)    \n",
    "        \n",
    "            idx_sv = ((t * ((X @ w) + b)) < 1).ravel()\n",
    "            X_sv = X[idx_sv]\n",
    "            t_sv = t[idx_sv]\n",
    "            \n",
    "            self.loss = ((0.5 * (w.T @ w))+ (self.C*(np.sum(1- (t_sv * (X_sv @ w))) - b* np.sum(t_sv)))).item()\n",
    "            #self.loss = ((0.5 * (w.T @ w))+ (self.C*(np.sum(1- X_sv@w) - b* np.sum(t_sv)))).item()\n",
    "            #self.loss = ((0.5 * (w.T @ w))+ (self.C*np.sum((np.maximum(0,1-t*(X@w+b)))))).item()\n",
    "            # print(self.loss)\n",
    "            dw = w - (self.C * np.sum(t_sv * X_sv))\n",
    "            db = -self.C * np.sum(t_sv)\n",
    "            \n",
    "            self.w = w - self.learning_rate * dw\n",
    "            self.b = b - self.learning_rate * db\n",
    "            self.support_vectors_ = X_sv\n",
    "            ## Assing intercept,Coef and Support vec\n",
    "            \n",
    "            if self.early_stopping:\n",
    "                t_val = (2 * y_valid - 1)[:,None]\n",
    "                self.validation_score = ((0.5 * (self.w.T @ self.w))+ (self.C * (np.sum(1- (X_valid @ self.w)) - b * np.sum(t_val)))).item()\n",
    "                val_loss.append(self.validation_score)\n",
    "                # print(\"self.validation_score\", self.validation_score)\n",
    "                # print(\"self.loss\", self.loss)\n",
    "                if (self.validation_score > validation_score):\n",
    "                    print(f'\\nEarly Stopping at : {epch_counter}\\n')\n",
    "                    plt.plot(range(epch_counter), val_loss, \"--\", color='darkorange', lw = 2)\n",
    "                    plt.xlabel(\"epoch\")\n",
    "                    plt.ylabel(\"Loss : J(w)\")\n",
    "                    break\n",
    "            \n",
    "            if (self.tol != None) and (np.abs(self.loss - loss) < self.tol):         \n",
    "                break\n",
    "                    \n",
    "        self.coef_ = np.array([w])\n",
    "        self.intercept_ = np.array([b])\n",
    "        \n",
    "    def predict(self,X):\n",
    "        return ((X@self.coef_[0] + self.intercept_[0]) >= 1).astype(np.int)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sFold(folds,data,labels,model,error_fuction,**model_args):\n",
    "    if(labels.shape == (labels.shape[0],)):\n",
    "        labels = np.expand_dims(labels,axis=1)\n",
    "    dataset = np.concatenate([data,labels],axis=1)\n",
    "    s_part = s_partition(dataset,folds)\n",
    "    pred_y = []\n",
    "    true_y = []\n",
    "    err_func = []\n",
    "    for idx,val in enumerate(s_part):\n",
    "        test_y = val[:,-1]\n",
    "        #test_y = np.expand_dims(test_y, axis=1)\n",
    "        test = val[:,:-1]\n",
    "        train = np.concatenate(np.delete(s_part,idx,0))\n",
    "        label = train[:,-1]\n",
    "        train = train[:,:-1]        \n",
    "        #model.fit(train,label,**model_args) \n",
    "        model.fit(train,label)  \n",
    "        pred = model.predict(test)\n",
    "        \n",
    "        pred_y.append(pred)\n",
    "        true_y.append(test_y[:,None])\n",
    "        err_func.append(error_fuction(pred,test_y[:,None]))\n",
    "    avg_error = np.array(err_func).mean()\n",
    "    result = {'Expected labels':true_y, 'Predicted labels': pred_y,'Average error':avg_error }\n",
    "    return result\n",
    "\n",
    "def s_partition(x,s):\n",
    "    return np.array_split(x,s)\n",
    "\n",
    "def accuracy(x,y):\n",
    "    x,y = np.array(x),np.array(y)\n",
    "    pred = (x == y).astype(np.int)\n",
    "    return pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findOptimalSVM():\n",
    "    validation_accuracy = np.empty((4,6,2,5,5))\n",
    "    learning_rate_init = [0.1 , 0.01, 0.001, 0.0001]\n",
    "    C = [0.01,0.5 , 1 , 10, 50, 100]\n",
    "    learning_rate= ['constant', 'adaptive']\n",
    "    max_iter = [10, 35, 50, 100, 500]\n",
    "    tol = [None, 0.1, 1, 0.01, 5]\n",
    "    maxScore = 0\n",
    "    for i,lri in enumerate(learning_rate_init):\n",
    "        for j,c in enumerate(C):\n",
    "            for k,lr in enumerate(learning_rate):\n",
    "                for l,mi in enumerate(max_iter):\n",
    "                    for m,tl in enumerate(tol):\n",
    "                        model_args = {'learning_rate' : lr,'C':c,'learning_rate_init' : lri,'max_iter' : mi,'tol' : tl}\n",
    "                        lrSvM = Linear_SVC(**model_args) \n",
    "                        result = sFold(5,X,y,lrSvM, error_fuction = accuracy,**model_args)\n",
    "                        validation_accuracy[i,j,k,l,m] = result['Average error']\n",
    "                        if validation_accuracy[i,j,k,l,m] > maxScore:\n",
    "                            maxScore = validation_accuracy[i,j,k,l,m]\n",
    "                            index = [i,j,k,l,m]\n",
    "\n",
    "    print(index)\n",
    "    a,b,c,d,e= index\n",
    "    print('optimal learning_rate_init: ',learning_rate_init[a])\n",
    "    print('optimal C: ',C[b])\n",
    "    print('optimal learning_rate: ',learning_rate[c])\n",
    "    print('optimal max_iter',max_iter[d])\n",
    "    print('optimal tol',tol[e])\n",
    "    print('optimal value',validation_accuracy[a,b,c,d,e])\n",
    "    opt_dic = {'learning_rate' : learning_rate[c],'C':C[b],'learning_rate_init' : learning_rate_init[a],'max_iter' : max_iter[d],'tol' : tol[e]}   \n",
    "    return opt_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 0, 1, 0]\n",
      "optimal learning_rate_init:  0.01\n",
      "optimal C:  50\n",
      "optimal learning_rate:  constant\n",
      "optimal max_iter 35\n",
      "optimal tol None\n",
      "optimal value 0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "opt_mod = findOptimalSVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear_SVC(**opt_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary_support_vectors(svm_clf, X):\n",
    "    \n",
    "    xmin, xmax = X.min() - 1, X.max() + 1\n",
    "    w = svm_clf.coef_[0]\n",
    "    b = svm_clf.intercept_[0]\n",
    "    \n",
    "\n",
    "    # At the decision boundary, w1*x1 + w2*x2 + b = 0\n",
    "    # => x2 = -(b + w1* x1)/w1\n",
    "    x1 = np.linspace(xmin, xmax, 100)    \n",
    "    decision_boundary = -(b + w[0]*x1)/w[1]\n",
    "    shifting_factor_for_margin = 1/w[1]\n",
    "    upper_margin = decision_boundary + shifting_factor_for_margin\n",
    "    lower_margin = decision_boundary - shifting_factor_for_margin\n",
    "\n",
    "    svs = svm_clf.support_vectors_\n",
    "    plt.scatter(svs[:, 0], svs[:, 1], s=200, facecolors='g', label=\"Support Vectors\")\n",
    "    plt.plot(x1, decision_boundary, \"k-\", linewidth=2)\n",
    "    plt.plot(x1, upper_margin, \"k--\", linewidth=2)\n",
    "    plt.plot(x1, lower_margin, \"k--\", linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(X_train[:, 0][y_train==1], X_train[:, 1][y_train==1], \"bo\", label=\"Class 0\")\n",
    "plt.plot(X_train[:, 0][y_train==0], X_train[:, 1][y_train==0], \"ro\", label=\"Class 1\")\n",
    "\n",
    "decision_boundary_support_vectors(model, X_train)\n",
    "\n",
    "plt.xlabel(\"x1\", fontsize=14)\n",
    "plt.ylabel(\"x2\", fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=14)\n",
    "plt.title(\"BGD_linearSVM\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_train)\n",
    "print(\"Train Accuracy: \", accuracy_score(pred, y_train))\n",
    "\n",
    "\n",
    "y_test_predicted = model.predict(X_test)\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test_predicted, y_test))\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X_train,y_train\n",
    "model_early = Linear_SVC(**opt_mod, early_stopping = True, validation_fraction=0.2)\n",
    "model_early.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_early.coef_,model_early.intercept_)\n",
    "support_vectors = model_early.support_vectors_\n",
    "support_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bo\", label=\"Class 0\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"ro\", label=\"Class 1\")\n",
    "\n",
    "decision_boundary_support_vectors(model_early, X)\n",
    "\n",
    "plt.xlabel(\"x1\", fontsize=14)\n",
    "plt.ylabel(\"x2\", fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=14)\n",
    "plt.title(\"BGD_linearSVM\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_early.predict(X)\n",
    "print(\"Train Accuracy: \", accuracy_score(pred, y))\n",
    "\n",
    "\n",
    "y_test_predicted = model_early.predict(X_test)\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test_predicted, y_test))\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
